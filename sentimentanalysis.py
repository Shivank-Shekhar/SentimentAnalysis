# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G1TDai9J0TMaysq0hd1Un2qEme3sifMU
"""

! pip install newsapi-python
from newsapi import NewsApiClient
newsapi = NewsApiClient(api_key='514d413ea7ae4bb69e4b019606e15882')
all_articles = newsapi.get_everything(q='',
                                      sources='bbc-news,the-verge',
                                      domains='bbc.co.uk,techcrunch.com',
                                      from_param='2022-12-01',
                                      to='2022-12-02',
                                      language='en',
                                      sort_by='relevancy',
                                      page=2)
all_articles

all_articles['articles'][0]

for i in range(all_articles['totalResults']):
  title= all_articles['articles'][i]['title']
  news = all_articles['articles'][i]['content']
  print(title+'\n'+news+'\n'+'\n')

import pandas as pd

df = pd.DataFrame(all_articles['articles'])
df=df.drop(['source', 'author', 'url', 'urlToImage', 'publishedAt', 'description'], axis=1)
df.head()

label= [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1]
df['label']=label
df.head()
#print(df['label'].sum())

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk_stopwords = set(stopwords.words('english'))


from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
sklearn_stopwords = set(ENGLISH_STOP_WORDS)


combined_stopwords = nltk_stopwords.union(sklearn_stopwords)

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

!pip install contractions
import contractions

df['content'][63].split()

nltk.download('wordnet')
nltk.download('omw-1.4')
new_sent = ''
for token in df['content'][63].split():
    new_sent = new_sent + lemmatizer.lemmatize(token.lower()) + ' '

new_sent

def news_cleaner_without_stopwords(text):
    new_text = re.sub(r"'s\b", " is", text)
  
    new_text = contractions.fix(new_text)    

    new_text = re.sub(r"\W", " ", new_text)

    new_text = re.sub(r"[^a-zA-Z_]", " ", new_text)

    new_text = new_text.lower().strip()
    
    cleaned_text = ''
    for token in new_text.split():
        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '
    
    return cleaned_text

cleaned_news = []  
for ele in df['content']:
    cleaned_news.append(news_cleaner_without_stopwords(ele))

cleaned_news[:5]

df['cleaned_news_w/o_SW'] = cleaned_news
df.head()

all_words = []
for t in df['content']:
    all_words.extend(t.split())

print(all_words[:50])
len(set(all_words)) # this is the number of unique words in the list

import matplotlib.pyplot as plt

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

all_words = []
for t in df['cleaned_news_w/o_SW']:
    all_words.extend(t.split())

print(all_words[:50])

len(set(all_words)) # this is the number of unique words in the list

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

def news_cleaner_with_stopwords(text):
    new_text = re.sub(r"'s\b", " is", text)
  
    new_text = contractions.fix(new_text)    
      
    new_text = new_text.lower().strip()
    
    new_text = [token for token in new_text.split() if token not in combined_stopwords]
    
    new_text = [token for token in new_text if len(token)>2]
    
    cleaned_text = ''
    for token in new_text:
        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '
    
    return cleaned_text

cleaned_news = list(df['content'].apply(news_cleaner_with_stopwords))
print(cleaned_news[:10])

df['cleaned_news_with_SW'] = cleaned_news
df.head()

all_words = []
for t in df['cleaned_news_with_SW']:
    all_words.extend(t.split())

print(all_words[:50])

# Frequency Distribution
freq_dist = nltk.FreqDist(all_words)

plt.figure(figsize=(12,5))
plt.title('Top 25 most common words')
plt.xticks(fontsize=15)

freq_dist.plot(25, cumulative=False)

plt.show()

len(set(all_words))

from sklearn.feature_extraction.text import CountVectorizer

CV = CountVectorizer()
CV_features = CV.fit_transform(df['cleaned_news_w/o_SW'])

CV_features.shape

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(CV_features, df['label'], test_size=0.25, stratify=df['label'], random_state=1000)

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(solver='liblinear')
LR.fit(X_train, y_train)

print(LR.score(X_train, y_train))  # train score)
print(LR.score(X_test, y_test))   # test score)

LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)
LR1.fit(X_train, y_train)

print(LR1.score(X_train, y_train))  # train score)
print(LR1.score(X_test, y_test))   # test score)

from sklearn.neighbors import KNeighborsClassifier                 #import the algorithm
kmodel = KNeighborsClassifier(n_neighbors=9) 


kmodel.fit(X_train,y_train)      
print(kmodel.score(X_test,y_test))
print(kmodel.score(X_train,y_train))

kmodel.fit(X_train,y_train)      
print(kmodel.score(X_test,y_test))
print(kmodel.score(X_train,y_train))